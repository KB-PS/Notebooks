{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.connector import connect\n",
    "import data_diff\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sflk_user = \"SAPI_WORKSPACE_971362817\"\n",
    "sflk_password = \"{{PWD FOR WORKSPACE}}\"\n",
    "sflk_account = \"owb79125.us-east-1\"\n",
    "\n",
    "sflk_database = \"SAPI_9609\"\n",
    "sflk_schema = \"WORKSPACE_971362817\"\n",
    "sflk_warehouse = \"KEBOOLA\"\n",
    "\n",
    "keboola_project_id = \"SAPI_9609\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conn():\n",
    "    \n",
    "    engine = create_engine(\n",
    "        'snowflake://{user}:{password}@{account_identifier}/{database_name}/{schema_name}?warehouse={warehouse_name}&role={role_name}'.format(\n",
    "            user=sflk_user,\n",
    "            password=sflk_password,\n",
    "            account_identifier=sflk_account,\n",
    "            database_name=sflk_database,\n",
    "            schema_name=sflk_schema,\n",
    "            warehouse_name=sflk_warehouse,\n",
    "            role_name=sflk_user\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        connection = engine.connect()\n",
    "        results = connection.execute('select current_version()').fetchone()\n",
    "        print(results[0])\n",
    "    finally:\n",
    "        connection.close()\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(branch_id: str, prod_table_name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    String operator - handling Keboola storage naming convention\n",
    "    Returns the table names for prod and dev environments based on the given branch ID.\n",
    "    \"\"\"\n",
    "\n",
    "    table_name_tmp = prod_table_name\n",
    "\n",
    "    # figuring out and removing the storage stage for now\n",
    "    keboola_storage_ids = [\"in.c-\", \"out.c-\"]\n",
    "    storage_stage = \"\"\n",
    "    for word in keboola_storage_ids:\n",
    "        if word in table_name_tmp:\n",
    "            #print(f\"Found '{word}' in '{table_name}'\")\n",
    "            storage_stage = word\n",
    "            table_name_tmp = table_name_tmp.replace(word, \"\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #print(storage_stage)\n",
    "\n",
    "    #replace comment, we will put them back\n",
    "    table_name_tmp = table_name_tmp.replace('\"', '')\n",
    "    #print(table_name_tmp)\n",
    "    \n",
    "    # Split the table name into schema and table parts\n",
    "    schema_name, table_name = table_name_tmp.split(\".\")\n",
    "    #print(storage_stage, schema_name, table_name)\n",
    "\n",
    "    # Prod Schema\n",
    "    prod_schema_name = f'\"{storage_stage}{schema_name}\"'\n",
    "    #print(prod_schema_name)\n",
    "    \n",
    "    # Construct the dev environment schema name by appending the branch ID to the prod schema name\n",
    "    dev_schema_name = f'\"{storage_stage}{branch_id}-{schema_name}\"'\n",
    "    \n",
    "    # Construct the dev environment table name by appending the branch ID to the prod table name\n",
    "    dev_table_name = f'{dev_schema_name}.\"{table_name}\"'\n",
    "    \n",
    "    #\"\"\"\n",
    "    logging.info(\"Dev table id: {}\".format(dev_table_name))\n",
    "    logging.info(\"Prod table id: {}\".format(prod_table_name))\n",
    "    logging.info(\"Dev schema name: {}\".format(dev_schema_name))\n",
    "    logging.info(\"Prod schema name: {}\".format(prod_schema_name))\n",
    "    print(\"Dev table id: {}\".format(dev_table_name))\n",
    "    print(\"Prod table id:\", prod_table_name)\n",
    "    print(\"Dev schema name:\", dev_schema_name)\n",
    "    print(\"Prod schema name:\", prod_schema_name)\n",
    "    #\"\"\"\n",
    "\n",
    "    # Return both table names as a tuple\n",
    "    return (dev_table_name, prod_table_name, dev_schema_name, prod_schema_name)\n",
    "\n",
    "\n",
    "def check_columns(table_name_prod, schema_name_prod):\n",
    "    \"\"\"\n",
    "    Connect to Snowflake and get column list\n",
    "    By loading data sample in pandas\n",
    "    # https://docs.snowflake.com/developer-guide/python-connector/python-connector-pandas\n",
    "    \"\"\"\n",
    "\n",
    "    sflk_dataset = table_name_prod.split(\".\")[2]\n",
    "    sflk_dataset = sflk_dataset.replace('\"','')\n",
    "\n",
    "\n",
    "\n",
    "    conn = connect(\n",
    "            user=sflk_user,\n",
    "            password=sflk_password,\n",
    "            account=sflk_account\n",
    "        )\n",
    "\n",
    "    dataset = '\"{database_name}\".\"{schema_name}\".\"{dataset_name}\"'.format(\n",
    "            database_name=sflk_database,\n",
    "            schema_name=schema_name_prod,\n",
    "            dataset_name=sflk_dataset\n",
    "        )\n",
    "\n",
    "\n",
    "    #sql = 'SELECT * FROM {} LIMIT 10'.format(dataset)\n",
    "    sql = 'DESCRIBE TABLE {}'.format(dataset)\n",
    "\n",
    "    logging.info(\"Executing Query: {}\".format(sql))\n",
    "    #print(\"Executing Query: {}\".format(sql))\n",
    "\n",
    "    # Execute SQL\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "\n",
    "    # Convert output to a dataframe\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=['name', 'type', 'kind', 'null', 'default', 'primary key', 'unique key', 'check', 'expression', 'comment', 'policy name'])\n",
    "\n",
    "    #df = cursor.fetch_pandas_all()\n",
    "    cursor.close()\n",
    "\n",
    "    #col_list = df.columns.tolist()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cx/dyw17k8n62l401jn8161bnfr0000gn/T/ipykernel_78588/1942719248.py:16: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  results = connection.execute('select current_version()').fetchone()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.15.0\n"
     ]
    }
   ],
   "source": [
    "test_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_id = \"719410\"\n",
    "table_name_prod = '\"in.c-data-diff-prep\".\"orders\"'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev table id: \"in.c-719410-data-diff-prep\".\"orders\"\n",
      "Prod table id: \"in.c-data-diff-prep\".\"orders\"\n",
      "Dev schema name: \"in.c-719410-data-diff-prep\"\n",
      "Prod schema name: \"in.c-data-diff-prep\"\n",
      "Available columns: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', '_timestamp']\n",
      "Timestamp columns: ['_timestamp']\n",
      "Primary Key columns: ['order_id', 'customer_id']\n",
      "Other columns: ['order_delivered_customer_date', 'order_purchase_timestamp', 'order_status', 'order_approved_at', 'order_estimated_delivery_date', 'order_delivered_carrier_date']\n",
      "Table name: orders\n"
     ]
    }
   ],
   "source": [
    "## Table Prep\n",
    "\n",
    "dev_table_id, prod_table_id, dev_schema_name, prod_schema_name = get_table_names(branch_id, table_name_prod)\n",
    "\n",
    "## grab table info from describe\n",
    "schema_name_prod = prod_schema_name.replace('\"','')\n",
    "table_describe = check_columns(table_name_prod, schema_name_prod)\n",
    "schema_name_dev = dev_schema_name.replace('\"','')\n",
    "\n",
    "#print(table_describe)\n",
    "\n",
    "## column list\n",
    "col_list = table_describe.iloc[:, 0].tolist()\n",
    "logging.info(\"Available columns: {}\".format(col_list))\n",
    "print(\"Available columns: {}\".format(col_list))\n",
    "\n",
    "# Filter rows containing \"TIMESTAMP\" in the \"type\" column\n",
    "ts_filtered = table_describe[table_describe[\"type\"].str.contains(\"TIMESTAMP\")]\n",
    "ts_filtered = ts_filtered.iloc[:, 0].tolist()\n",
    "logging.info(\"Timestamp columns: {}\".format(ts_filtered))\n",
    "print(\"Timestamp columns: {}\".format(ts_filtered))\n",
    "\n",
    "# Filter rows identified as Primary Key in the \"type\" column\n",
    "pk_filtered = table_describe[table_describe[\"primary key\"].str.contains(\"Y\")]\n",
    "pk_filtered = pk_filtered.iloc[:, 0].tolist()\n",
    "logging.info(\"Primary Key columns: {}\".format(pk_filtered))\n",
    "print(\"Primary Key columns: {}\".format(pk_filtered))\n",
    "\n",
    "# Lets figure out leftover columns\n",
    "# all - pks\n",
    "other_col = list(set(col_list).difference(set(pk_filtered)))\n",
    "# diff - timestamp\n",
    "if \"_timestamp\" in col_list:\n",
    "    diff_col = list(set(other_col).difference(set([\"_timestamp\"])))\n",
    "else:\n",
    "    print(\"Column '_timestamp' has not been found in the dataset. Has the table been created within Keboola platform?\")\n",
    "    logging.warning(\"Column '_timestamp' has not been found in the dataset. Has the table been created within Keboola platform?\")\n",
    "    diff_col = other_col\n",
    "logging.info(\"Other columns: {}\".format(diff_col))\n",
    "print(\"Other columns: {}\".format(diff_col))\n",
    "\n",
    "# plain table name\n",
    "table_name = table_name_prod.split(\".\")[2]\n",
    "table_name = table_name.replace('\"','')\n",
    "logging.info(\"Table name: {}\".format(table_name))\n",
    "print(\"Table name: {}\".format(table_name))\n",
    "\n",
    "# This is a result checking columns in the previous step, basically list of table columns except \"_timestamp\" and PKs\n",
    "sflk_key_columns = tuple(pk_filtered)\n",
    "sflk_dataset_columns = tuple(diff_col)\n",
    "# ('order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date','order_delivered_customer_date', 'order_estimated_delivery_date')\n",
    "\n",
    "\n",
    "## Forming Data Diff conn directories\n",
    "table_source_dict = {\n",
    "    \"driver\": \"snowflake\",\n",
    "    \"user\": sflk_user,\n",
    "    \"password\": sflk_password,\n",
    "    \"account\": sflk_account,\n",
    "    \"role\": sflk_user,\n",
    "    \"warehouse\": sflk_warehouse,\n",
    "    \"database\": keboola_project_id,\n",
    "    \"schema\": schema_name_prod\n",
    "}\n",
    "table_source_name = table_name\n",
    "\n",
    "table_destination_dict = {\n",
    "    \"driver\": \"snowflake\",\n",
    "    \"user\": sflk_user,\n",
    "    \"password\": sflk_password,\n",
    "    \"account\": sflk_account,\n",
    "    \"role\": sflk_user,\n",
    "    \"warehouse\": sflk_warehouse,\n",
    "    \"database\": keboola_project_id,\n",
    "    \"schema\": schema_name_dev\n",
    "}\n",
    "\n",
    "table_destination_name = table_name\n",
    "\n",
    "table_source = data_diff.connect_to_table(\n",
    "    table_source_dict\n",
    "    ,table_source_name\n",
    "    ,key_columns=sflk_key_columns\n",
    "    ,update_column=\"_timestamp\"\n",
    "    ,extra_columns=sflk_dataset_columns\n",
    "    )\n",
    "table_destination=data_diff.connect_to_table(\n",
    "    table_destination_dict\n",
    "    ,table_destination_name\n",
    "    ,key_columns=sflk_key_columns\n",
    "    ,update_column=\"_timestamp\"\n",
    "    ,extra_columns=sflk_dataset_columns\n",
    "    )\n",
    "\n",
    "#print(\"----------------------\")\n",
    "#print(\"Using data:\")\n",
    "#print(table_source, \"\\n\", table_destination)\n",
    "logging.info(\"Using data:\")\n",
    "logging.debug(table_source)\n",
    "logging.debug(table_destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_delivered_customer_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_purchase_timestamp' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_approved_at' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_estimated_delivery_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_delivered_carrier_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_delivered_customer_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_purchase_timestamp' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_approved_at' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_estimated_delivery_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n",
      "WARNING:hashdiff_tables:[Snowflake] Column 'order_delivered_carrier_date' of type 'Text()' has no compatibility handling. If encoding/formatting differs between databases, it may result in false positives.\n"
     ]
    }
   ],
   "source": [
    "## Lets diff\n",
    "#print(\"----------------------\")\n",
    "#print(\"Running DIFF\")\n",
    "logging.info(\"Running DIFF\")\n",
    "diff_result = data_diff.diff_tables(table_source, table_destination, threaded=True, max_threadpool_size=6)\n",
    "diff_list = list(diff_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "printing diff sample:\n",
      "('-', ('00042b26cf59d7ce69dfabb4e55b4fd9', '58dbd0b2d70206bf40e62cd34e84d795', '2023-04-25 21:01:14.000000', '2017-03-01 16:42:31.000', '2017-02-04 13:57:51.000', 'delivered', '2017-02-04 14:10:13.000', '2017-03-17 00:00:00.000', '2017-02-16 09:46:09.000'))\n",
      "('-', ('000e906b789b55f64edcb1f84030f90d', '6a3b2fc9f270df258605e22bef19fd88', '2023-04-25 21:01:14.000000', '2017-12-09 17:27:23.000', '2017-11-21 18:54:23.000', 'delivered', '2017-11-21 19:09:02.000', '2017-12-07 00:00:00.000', '2017-11-22 20:46:54.000'))\n",
      "('-', ('0010b2e5201cc5f1ae7e9c6cc8f5bd00', '57ef317d4818cb42680fc9dfd13867ce', '2023-04-25 21:01:14.000000', '2017-09-23 13:21:21.000', '2017-09-11 17:39:33.000', 'delivered', '2017-09-11 18:04:37.000', '2017-09-27 00:00:00.000', '2017-09-12 17:02:57.000'))\n",
      "('-', ('00119ff934e539cf26f92b9ef0cdfed8', '7dd2e283f47deac853cf70f3b63c8d86', '2023-04-25 21:01:14.000000', '2017-08-16 17:29:59.000', '2017-08-06 00:42:49.000', 'delivered', '2017-08-07 00:35:12.000', '2017-08-31 00:00:00.000', '2017-08-07 21:12:34.000'))\n",
      "('-', ('00137e170939bba5a3134e2386413108', '7b63cba66b8e6d002e94d0990c1e2868', '2023-04-25 21:01:14.000000', '2017-12-11 19:12:55.000', '2017-11-24 16:50:38.000', 'delivered', '2017-11-25 06:30:55.000', '2017-12-18 00:00:00.000', '2017-11-28 19:43:52.000'))\n",
      "('-', ('001dbc16dc51075e987543d23a0507c7', '698a74f33469466fa4172e829505d1c6', '2023-04-25 21:01:14.000000', '2017-02-13 13:17:47.000', '2017-01-28 13:17:57.000', 'delivered', '2017-01-28 13:32:16.000', '2017-03-20 00:00:00.000', '2017-02-01 15:59:46.000'))\n",
      "('-', ('00259a44fcad3fc0474329e925d14fc3', '7715f80f77f2ebf4583f97a0a6a7548b', '2023-04-25 21:01:14.000000', '2018-01-04 13:32:46.000', '2017-12-27 17:52:11.000', 'delivered', '2017-12-27 17:59:35.000', '2018-01-22 00:00:00.000', '2017-12-28 21:53:36.000'))\n",
      "('-', ('002691433f09002ac9ca0c4e8dbb8ead', '630b8c2f4134e83812d827e8006caa85', '2023-04-25 21:01:14.000000', '2017-02-07 13:04:37.000', '2017-02-02 21:18:03.000', 'delivered', '2017-02-02 21:30:13.000', '2017-03-03 00:00:00.000', '2017-02-03 12:34:49.000'))\n",
      "----------------------\n",
      "Number of observed differences: 45757\n",
      "----------------------\n",
      "General Stats:\n",
      "  observation  count\n",
      "0           -  45428\n",
      "1           +    329\n",
      "----------------------\n",
      "Relevant columns:\n",
      "['order_id', 'customer_id', '_timestamp', 'order_delivered_customer_date', 'order_purchase_timestamp', 'order_status', 'order_approved_at', 'order_estimated_delivery_date', 'order_delivered_carrier_date']\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------\")\n",
    "print(\"printing diff sample:\")\n",
    "for a in diff_list[:8]:\n",
    "    print(a)\n",
    "\n",
    "print(\"----------------------\")\n",
    "diff_df = pd.DataFrame(diff_list)\n",
    "print(\"Number of observed differences: {}\".format(len(diff_df)))\n",
    "print(\"----------------------\")\n",
    "# Get unique value counts as a DataFrame\n",
    "value_counts = pd.DataFrame(diff_df[0].value_counts()).reset_index()\n",
    "# Rename columns\n",
    "value_counts.columns = [\"observation\", \"count\"]\n",
    "# Print the resulting DataFrame\n",
    "print(\"General Stats:\")\n",
    "print(value_counts)\n",
    "print(\"----------------------\")\n",
    "print(\"Relevant columns:\")\n",
    "print(table_source.relevant_columns)\n",
    "print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
